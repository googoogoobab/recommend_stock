{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8543522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64819795",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/\"\n",
    "search_word_list=[\"palantir\",\"Tesla\"]\n",
    "search_word=search_word_list[1]\n",
    "\n",
    "start_date=\"2018-04-19\"\n",
    "last_date=\"2020-01-13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01cc1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_twitter_dat(path, search_word, start_date, last_date):\n",
    "    results = pd.DataFrame([])\n",
    "    for date in pd.date_range(start=start_date, end=last_date):\n",
    "        results = results.append(pd.read_csv(path +\n",
    "                                            search_word +\n",
    "                                            \"_\" +\n",
    "                                            date.strftime('%Y-%m-%d') + \".csv\", index_col=0, parse_dates=[\"date\"]))\n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "concat_data=concat_twitter_dat(path,search_word,start_date,last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a141b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpTweet(txt):\n",
    "    txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n",
    "    txt = re.sub(r'#', '', txt)\n",
    "    txt = re.sub(r'RT : ', '', txt)\n",
    "    txt = re.sub(r'https?:|/|/[A-Za-z0-9\\.\\/]+', '', txt)\n",
    "    return txt\n",
    "cleand_data=concat_data\n",
    "cleand_data[\"text\"]=concat_data[\"text\"].apply(cleanUpTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01fe9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTextSubjectivity(txt):\n",
    "    return TextBlob(txt).sentiment.subjectivity\n",
    "\n",
    "def getTextPolarity(txt):\n",
    "    return TextBlob(txt).sentiment.polarity\n",
    "\n",
    "def getTextAnalysis(a):\n",
    "    if a < 0:\n",
    "        return \"Negative\"\n",
    "    elif a == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "def emotiontDataAdd(twitter_data):\n",
    "    results = twitter_data.copy()\n",
    "    results['clean_text'] = results['text'].apply(cleanUpTweet)\n",
    "    results[\"polarity\"] = results['clean_text'].apply(getTextPolarity)\n",
    "    results[\"subjectivity\"] = results['clean_text'].apply(getTextSubjectivity)\n",
    "    results[\"emotion\"] = results['polarity'].apply(getTextAnalysis)\n",
    "    \n",
    "    return results\n",
    "\n",
    "prepare_data=emotiontDataAdd(cleand_data)\n",
    "prepare_data[\"date\"]=prepare_data[\"date\"].dt.tz_localize(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2e2afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDateType(stock_data):\n",
    "    results = stock_data.copy()\n",
    "    results[\"Date\"] = results.index.tz_convert('UTC')\n",
    "    results[\"Date\"] = results[\"Date\"].dt.tz_localize(None)\n",
    "    results = results.reset_index(drop=True)\n",
    "    return results\n",
    "\n",
    "interval=\"1D\"\n",
    "\n",
    "stock_data=pd.read_csv(path +\"stock_\" + start_date + \"_\" + last_date  + \"_\"+ interval + \".csv\")\n",
    "\n",
    "stock_data[\"Date\"]=pd.to_datetime(stock_data[\"Date\"],utc=True)\n",
    "stock_data.set_index('Date',drop=False, inplace=True)\n",
    "\n",
    "\n",
    "stock_data=convertDateType(stock_data)\n",
    "#stock_data.set_index('date',drop=False, inplace=True)\n",
    "#stock_data=stock_data.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1b79ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "def add_twitter_variables(stock_data,\n",
    "                         twitter_data,\n",
    "                         tweet_lag=60,\n",
    "                         subset_tweets_per=1):\n",
    "\n",
    "    interval = int((stock_data[\"Date\"][1] - stock_data[\"Date\"][0]).seconds / 60)\n",
    "    \n",
    "\n",
    "    results = stock_data.copy()\n",
    "    results[\"tw_count\"] = pd.NA     # number of tweets per interval\n",
    "    results[\"tw_mean\"] = pd.NA      # mean of number of tweets per subset_twets_per in interval\n",
    "    results[\"tw_vola\"] = pd.NA      # volatility of number of tweets per subset_tweets_per in interval\n",
    "    results[\"tw_min\"] = pd.NA       # min number of tweets per subset_tweets_per in interval\n",
    "    results[\"tw_max\"] = pd.NA       # max number of tweets per subset_tweets_per in interval\n",
    "    results[\"tw_chars\"] = pd.NA     # avg number of characters of tweets per interval\n",
    "    results[\"tw_pola\"] =pd.NA      # avg polarity of tweets\n",
    "    results[\"tw_subj\"] = pd.NA     # avg subjectivity of tweets\n",
    "    results[\"tw_n_pos\"] = pd.NA     # number of positive tweets\n",
    "    results[\"tw_n_neg\"] = pd.NA     # number of negative tweets\n",
    "    results[\"tw_ratio_pos\"] =pd.NA # share of positive tweets\n",
    "    results[\"tw_ratio_neg\"] = pd.NA # share of negative tweets\n",
    "    \n",
    "\n",
    "    for i in range(0, stock_data.shape[0]):\n",
    "        current_time = stock_data.loc[i,\"Date\"]\n",
    "        \n",
    "        cond_1 = twitter_data[\"date\"] < (current_time + dt.timedelta(minutes=interval - tweet_lag))\n",
    "        cond_2 = twitter_data[\"date\"] >= (current_time - dt.timedelta(minutes=tweet_lag))\n",
    "        \n",
    "        twitter_subset = twitter_data.loc[cond_1 & cond_2, :].copy().reset_index(drop=True)\n",
    "        \n",
    "        results.loc[i, \"tw_count\"] = twitter_subset.shape[0]\n",
    "        \n",
    "\n",
    "        tweets_per = pd.Series(np.zeros(interval))\n",
    "        for x in np.arange(subset_tweets_per, interval + 1, subset_tweets_per):\n",
    "            sub_cond_1 = twitter_subset[\"date\"] >= (current_time + dt.timedelta(minutes=(int(x) - 1 - tweet_lag)))\n",
    "            sub_cond_2 = twitter_subset[\"date\"] < (current_time + dt.timedelta(minutes=(int(x) - tweet_lag)))\n",
    "            tweets_per[x - 1] = twitter_subset.loc[sub_cond_1 & sub_cond_2, :].shape[0]\n",
    "            \n",
    "        # Compute Variables \"tw_mean\", \"tw_vola\", \"tw_min\", \"tw_max\"\n",
    "        #results.loc[i, \"tw_mean\"] = tweets_per.mean()\n",
    "        #results.loc[i, \"tw_vola\"] = tweets_per.var()\n",
    "        #results.loc[i, \"tw_min\"] = tweets_per.min()\n",
    "        #results.loc[i, \"tw_max\"] = tweets_per.max()\n",
    "        \n",
    "        # Compute Variable \"tw_chars\"\n",
    "        twitter_subset[\"tw_chars\"] = twitter_subset.text.apply(len)\n",
    "        #results.loc[i, \"tw_chars\"] = twitter_subset[\"tw_chars\"].mean()\n",
    "        \n",
    "\n",
    "        # Content related variables\n",
    "        results.loc[i, \"tw_pola\"] = twitter_subset[\"polarity\"].mean()\n",
    "        results.loc[i, \"tw_subj\"] = twitter_subset[\"subjectivity\"].mean()\n",
    "        results.loc[i, \"tw_n_pos\"] = twitter_subset.loc[twitter_subset.emotion == \"Positive\", :]\n",
    "        results.loc[i, \"tw_n_neg\"] = twitter_subset.loc[twitter_subset.emotion == \"Negative\", :]\n",
    "        if twitter_subset.shape[0]!=0:\n",
    "            results.loc[i, \"tw_ratio_pos\"] = results.loc[i, \"tw_n_pos\"] / twitter_subset.shape[0]\n",
    "            results.loc[i, \"tw_ratio_neg\"] = results.loc[i, \"tw_n_neg\"] / twitter_subset.shape[0]\n",
    "        \n",
    "    return results\n",
    "variables_data=add_twitter_variables(stock_data,prepare_data)\n",
    "variables_data=variables_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3da48add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'tw_count',\n",
      "       'tw_mean', 'tw_vola', 'tw_min', 'tw_max', 'tw_chars', 'tw_pola',\n",
      "       'tw_subj', 'tw_n_pos', 'tw_n_neg', 'tw_ratio_pos', 'tw_ratio_neg'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#variables_data[\"date\"]=variables_data[\"date\"].values.astype(\"float64\")\n",
    "variables_data=variables_data.drop([\"Date\"],axis=1)\n",
    "#variables_data=variables_data.drop([\"date\"],axis=1)\n",
    "print(variables_data.columns)\n",
    "\n",
    "def makePersent(data):\n",
    "    result = data.copy()\n",
    "    for i in range(1,data.shape[0]):\n",
    "        result.loc[i,\"Open\"]=(data.loc[i,\"Open\"]-data.loc[i-1,\"Close\"])/data.loc[i-1,\"Close\"] *100\n",
    "        result.loc[i,\"High\"]=(data.loc[i,\"High\"]-data.loc[i-1,\"Close\"])/data.loc[i-1,\"Close\"] *100\n",
    "        result.loc[i,\"Low\"]=(data.loc[i,\"Low\"]-data.loc[i-1,\"Close\"])/data.loc[i-1,\"Close\"] *100\n",
    "        result.loc[i,\"Close\"]=(data.loc[i,\"Close\"]-data.loc[i-1,\"Close\"])/data.loc[i-1,\"Close\"] *100\n",
    "        result.loc[i,\"Adj Close\"]=(data.loc[i,\"Adj Close\"]-data.loc[i-1,\"Adj Close\"])/data.loc[i-1,\"Adj Close\"] *100\n",
    "        \n",
    "        result.loc[i,\"Volume\"]=(data.loc[i,\"Volume\"]-data.loc[i-1,\"Volume\"])/data.loc[i-1,\"Volume\"] *100\n",
    "        \n",
    "    return result\n",
    "        \n",
    "    \n",
    "    \n",
    "persent_data_data=makePersent(variables_data)\n",
    "\n",
    "# first volume of yfinace data is always 0. so second volume persent is inf. remove two row\n",
    "#sometimes last row have current date. not target date\n",
    "variables_data=persent_data_data.drop([persent_data_data.index[0]],axis=0)\n",
    "variables_data=variables_data.drop([variables_data.index[0]],axis=0)\n",
    "variables_data=variables_data.drop([variables_data.index[-1]],axis=0)\n",
    "variables_data=variables_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6145a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale\n",
    "scaler = MinMaxScaler()\n",
    "variables_data[[\"tw_count\",\"tw_mean\",\"tw_vola\",\"tw_min\",\"tw_max\",\"tw_chars\"]]=scaler.fit_transform(variables_data[[\"tw_count\",\"tw_mean\",\"tw_vola\",\"tw_min\",\"tw_max\",\"tw_chars\"]])\n",
    "only_stock_data_training = variables_data.iloc[:, 0:6].copy()\n",
    "\n",
    "train_set, test_set = train_test_split(variables_data, test_size = 0.3,shuffle=False)\n",
    "only_stock_train_set, only_stock_test_set = train_test_split(only_stock_data_training, test_size = 0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "602aa11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a67c38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLSTMDataSet(data_training,data_test\n",
    "                    ,step_size = 5):\n",
    "    train_data = []\n",
    "    target_data = []\n",
    "\n",
    "    for i in range(step_size, data_training.shape[0]):\n",
    "        train_data.append(data_training.loc[i - step_size:i-1])\n",
    "        target_data.append(data_training.loc[i, \"Close\"])\n",
    "    train_data, target_data = np.array(train_data), np.array(target_data)\n",
    "\n",
    "    #test data\n",
    "    test_train_data = []\n",
    "    test_target_data = []\n",
    "    \n",
    "    data_test=data_test.reset_index(drop=True)\n",
    "    for i in range(step_size, data_test.shape[0]):\n",
    "        test_train_data.append(data_test.loc[i - step_size:i-1])\n",
    "        test_target_data.append(data_test.loc[i, \"Close\"])\n",
    "    test_train_data, test_target_data = np.array(test_train_data), np.array(test_target_data)\n",
    "    \n",
    "    return train_data, target_data, test_train_data, test_target_data\n",
    "\n",
    "\n",
    "train_data, target_data, test_train_data, test_target_data=makeLSTMDataSet(train_set, test_set)\n",
    "only_stock_train_data, only_stock_target_data, only_stock_test_train_data, only_stock_test_target_data=makeLSTMDataSet(only_stock_train_set, only_stock_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b9b21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create  Model\n",
    "def model_wrapper_get_input_size(input_data):\n",
    "    def build_model(hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        hidden = hp.Int(\"num_layers\",\n",
    "                    min_value = 0,\n",
    "                    max_value = 3)\n",
    "        activations = hp.Choice(\"activation\",\n",
    "                                        values = [\"relu\", \"tanh\", \"sigmoid\"],\n",
    "                                        default = \"relu\")\n",
    "        model.add(LSTM(units = hp.Int(\"units\",\n",
    "                                    min_value = 18,\n",
    "                                    max_value = 128,\n",
    "                                    step = 32),\n",
    "                    activation = activations,\n",
    "                    input_shape = (input_data.shape[1], input_data.shape[2]),\n",
    "                    return_sequences = True if hidden > 0 else False))\n",
    "        \n",
    "        if hidden > 0:\n",
    "            for layer in range(hidden):\n",
    "                model.add(Dropout(hp.Float(\"dropout_hidden\" + str(layer + 1),\n",
    "                                        min_value = 0.1,\n",
    "                                        max_value = 0.9,\n",
    "                                        step = 0.1)))\n",
    "                \n",
    "                model.add(LSTM(units = hp.Int(\"units_hidden\" + str(layer + 1),\n",
    "                                            min_value = 18,\n",
    "                                            max_value = 64,\n",
    "                                            step = 32),\n",
    "                            activation = activations,\n",
    "                            return_sequences = True if layer+1 != hidden else False))\n",
    "                \n",
    "        model.add(Dropout(hp.Float(\"dropout\",\n",
    "                                min_value = 0.1,\n",
    "                                max_value = 0.9,\n",
    "                                step = 0.1)))\n",
    "        model.add(Dense(units = 1))\n",
    "        \n",
    "        hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "        model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                        loss = 'mean_squared_error')\n",
    "            \n",
    "        return model\n",
    "    return build_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b44228a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getBestModel(trainData,targetData,projectName=\"recommend_stock\"):\n",
    "    tuner = RandomSearch(model_wrapper_get_input_size(trainData),\n",
    "                        objective = 'val_loss',\n",
    "                        max_trials = 10,\n",
    "                        executions_per_trial = 2,\n",
    "                        overwrite=False,\n",
    "                        directory = \"model_store\",\n",
    "                        project_name=projectName,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    early_stopping_cb = EarlyStopping(monitor='val_loss',patience = 10)\n",
    "\n",
    "    tuner.search(trainData,\n",
    "                targetData,\n",
    "                epochs = 1000,\n",
    "                batch_size = 32,\n",
    "                validation_split = 0.2,\n",
    "                callbacks = [early_stopping_cb])\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "    layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "    is {best_hps.get('learning_rate')}.\n",
    "    \"\"\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae687208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project model_store\\recommend_stock\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from model_store\\recommend_stock\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "    The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "    layer is 114 and the optimal learning rate for the optimizer\n",
      "    is 0.0001.\n",
      "    \n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 3s 43ms/step - loss: 38.9949 - val_loss: 12.0103\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 39.3081 - val_loss: 11.8547\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 39.1565 - val_loss: 11.7436\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 31.1456 - val_loss: 11.6714\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 41.1680 - val_loss: 11.5999\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 51.9856 - val_loss: 11.5191\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 33.8772 - val_loss: 11.4540\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 39.6592 - val_loss: 11.3889\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 31.5093 - val_loss: 11.3360\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 41.5341 - val_loss: 11.3026\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 54.0270 - val_loss: 11.2705\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 28.3073 - val_loss: 11.2435\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 37.6849 - val_loss: 11.2091\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 32.7508 - val_loss: 11.1861\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 41.2200 - val_loss: 11.1740\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 33.7228 - val_loss: 11.1564\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 39.8039 - val_loss: 11.1157\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 35.3402 - val_loss: 11.0574\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 28.9326 - val_loss: 11.0193\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 31.8224 - val_loss: 11.0042\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 49.8713 - val_loss: 10.9791\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 29.6229 - val_loss: 10.9796\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 24.7165 - val_loss: 11.0064\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 26.8350 - val_loss: 11.0149\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 34.2621 - val_loss: 10.9947\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 24.4197 - val_loss: 10.9770\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 22.5280 - val_loss: 10.9816\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 37.9836 - val_loss: 10.9655\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 22.2350 - val_loss: 10.9941\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 23.3519 - val_loss: 10.9801\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 26.0902 - val_loss: 10.9576\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 25.4722 - val_loss: 10.8730\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 28.2946 - val_loss: 10.7759\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 25.3946 - val_loss: 10.7675\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 21.8723 - val_loss: 10.7130\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 53.2252 - val_loss: 10.6927\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 26.8566 - val_loss: 10.6746\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 21.7906 - val_loss: 10.6543\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 26.8577 - val_loss: 10.6118\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 28.4408 - val_loss: 10.5609\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 19.5851 - val_loss: 10.5375\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 24.0932 - val_loss: 10.5182\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 19.8041 - val_loss: 10.5138\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 33.6226 - val_loss: 10.5034\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 23.7324 - val_loss: 10.4885\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 19.4830 - val_loss: 10.4826\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 36.8206 - val_loss: 10.4976\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 25.0085 - val_loss: 10.4885\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 18.2474 - val_loss: 10.5242\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 23.0014 - val_loss: 10.5180\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 40.5295 - val_loss: 10.5047\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 30.4068 - val_loss: 10.5084\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 53.5461 - val_loss: 10.4694\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 20.4981 - val_loss: 10.5229\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 20.6703 - val_loss: 10.5086\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 22.3780 - val_loss: 10.5496\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 17.9918 - val_loss: 10.5845\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 28.5879 - val_loss: 10.6302\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 17.0867 - val_loss: 10.6249\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 22.0199 - val_loss: 10.5538\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 19.7956 - val_loss: 10.5097\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 20.3196 - val_loss: 10.5145\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 23.6481 - val_loss: 10.5348\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 18.6428 - val_loss: 10.5331\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 19.5184 - val_loss: 10.5156\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 18.5807 - val_loss: 10.5098\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.3010 - val_loss: 10.4990\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 28.6743 - val_loss: 10.5164\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 18.2373 - val_loss: 10.5280\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 22.4602 - val_loss: 10.5221\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 18.7724 - val_loss: 10.5112\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.5582 - val_loss: 10.4959\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 35.1766 - val_loss: 10.4817\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 21.5646 - val_loss: 10.4479\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 24.2150 - val_loss: 10.4078\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 23.2137 - val_loss: 10.4712\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 25.9576 - val_loss: 10.4993\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 24.8913 - val_loss: 10.4843\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.7362 - val_loss: 10.4678\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 15.4978 - val_loss: 10.4405\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 17.0668 - val_loss: 10.3513\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 18.4399 - val_loss: 10.2472\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.5315 - val_loss: 10.1739\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 17.4331 - val_loss: 10.1467\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.4248 - val_loss: 10.0989\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.4221 - val_loss: 10.0441\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.9550 - val_loss: 10.0213\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 20.6197 - val_loss: 10.0478\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.4615 - val_loss: 10.0029\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.5997 - val_loss: 9.9938\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.8250 - val_loss: 9.9732\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.3972 - val_loss: 9.9412\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.5215 - val_loss: 9.8889\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.8097 - val_loss: 9.8770\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 18.4555 - val_loss: 9.8817\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.7696 - val_loss: 9.8869\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 17.1528 - val_loss: 9.8703\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.3489 - val_loss: 9.8649\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.9095 - val_loss: 9.8566\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.3804 - val_loss: 9.8369\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.8800 - val_loss: 9.8372\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.3084 - val_loss: 9.8460\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.8986 - val_loss: 9.8454\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.7714 - val_loss: 9.8437\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.8922 - val_loss: 9.8525\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 20.8153 - val_loss: 9.8515\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6731 - val_loss: 9.8464\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.5096 - val_loss: 9.8446\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.6215 - val_loss: 9.8408\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.5089 - val_loss: 9.8405\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.3619 - val_loss: 9.8465\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 15.4752 - val_loss: 9.8471\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.5202 - val_loss: 9.8445\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.8835 - val_loss: 9.8422\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.9584 - val_loss: 9.8400\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.8156 - val_loss: 9.8508\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.7702 - val_loss: 9.8634\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.4801 - val_loss: 9.9125\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.9320 - val_loss: 9.9281\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6605 - val_loss: 9.9239\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7861 - val_loss: 9.9173\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3803 - val_loss: 9.9136\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.1132 - val_loss: 9.9110\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.8334 - val_loss: 9.9092\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.9157 - val_loss: 9.9001\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.8169 - val_loss: 9.8973\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 18.5791 - val_loss: 9.8888\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6117 - val_loss: 9.8851\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.8496 - val_loss: 9.8832\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.0001 - val_loss: 9.8805\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.5287 - val_loss: 9.8781\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.8026 - val_loss: 9.8721\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.7334 - val_loss: 9.8672\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.2545 - val_loss: 9.8604\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.5803 - val_loss: 9.8570\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.5255 - val_loss: 9.8534\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.2783 - val_loss: 9.8491\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6443 - val_loss: 9.8469\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.1733 - val_loss: 9.8455\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.3736 - val_loss: 9.8465\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 21.2862 - val_loss: 9.8528\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.5978 - val_loss: 9.8620\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.5533 - val_loss: 9.8665\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.1271 - val_loss: 9.8695\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.5267 - val_loss: 9.8717\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.2694 - val_loss: 9.8737\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 15.8160 - val_loss: 9.8731\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.2385 - val_loss: 9.8732\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.5748 - val_loss: 9.8741\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.2495 - val_loss: 9.8631\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.8156 - val_loss: 9.8627\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.0216 - val_loss: 9.8671\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.5654 - val_loss: 9.8709\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.4819 - val_loss: 9.8740\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.2109 - val_loss: 9.8762\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.9953 - val_loss: 9.8772\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.0302 - val_loss: 9.8797\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.5693 - val_loss: 9.8829\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.1364 - val_loss: 9.8803\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 22.6100 - val_loss: 9.8719\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.5945 - val_loss: 9.8727\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6061 - val_loss: 9.8714\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.3247 - val_loss: 9.8713\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.4096 - val_loss: 9.8704\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.8714 - val_loss: 9.8669\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.8444 - val_loss: 9.8664\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.7907 - val_loss: 9.8682\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.8480 - val_loss: 9.8722\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.9304 - val_loss: 9.8712\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.9272 - val_loss: 9.8733\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 15.3656 - val_loss: 9.8762\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.7670 - val_loss: 9.8843\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.8063 - val_loss: 9.8798\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.5805 - val_loss: 9.8719\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.7091 - val_loss: 9.8827\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 14.6435 - val_loss: 9.8923\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.5995 - val_loss: 9.8951\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.5562 - val_loss: 9.8992\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.1996 - val_loss: 9.9022\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 15.3624 - val_loss: 9.9172\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.2075 - val_loss: 9.9218\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.9923 - val_loss: 9.9246\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8881 - val_loss: 9.9295\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.7460 - val_loss: 9.9384\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.9485 - val_loss: 9.9414\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.5157 - val_loss: 9.9445\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.5966 - val_loss: 9.9484\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0873 - val_loss: 9.9510\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.3996 - val_loss: 9.9528\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.0819 - val_loss: 9.9552\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.7807 - val_loss: 9.9548\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.4066 - val_loss: 9.9587\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.8052 - val_loss: 9.9621\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8884 - val_loss: 9.9633\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.2523 - val_loss: 9.9622\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.4150 - val_loss: 9.9655\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8806 - val_loss: 9.9815\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.3139 - val_loss: 9.9862\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.2352 - val_loss: 9.9887\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6854 - val_loss: 9.9894\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.7047 - val_loss: 9.9900\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.0505 - val_loss: 9.9897\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.1617 - val_loss: 9.9907\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.9498 - val_loss: 9.9912\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.6732 - val_loss: 9.9916\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.1154 - val_loss: 9.9934\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.7940 - val_loss: 9.9945\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.0442 - val_loss: 9.9967\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.2384 - val_loss: 9.9993\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.0549 - val_loss: 10.0015\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6549 - val_loss: 10.0035\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.8876 - val_loss: 10.0036\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3546 - val_loss: 10.0050\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.0502 - val_loss: 10.0056\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.4566 - val_loss: 10.0058\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.8450 - val_loss: 10.0077\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.6416 - val_loss: 10.0107\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.7529 - val_loss: 10.0132\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.6055 - val_loss: 10.0158\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 12.9973 - val_loss: 10.0164\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.5110 - val_loss: 10.0162\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.0367 - val_loss: 10.0156\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8850 - val_loss: 10.0143\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2181 - val_loss: 10.0139\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.7822 - val_loss: 10.0134\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.6433 - val_loss: 10.0137\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.1817 - val_loss: 10.0153\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5558 - val_loss: 10.0168\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1997 - val_loss: 10.0184\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8358 - val_loss: 10.0196\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 15.3032 - val_loss: 10.0220\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2103 - val_loss: 10.0231\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2786 - val_loss: 10.0237\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0994 - val_loss: 10.0250\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5478 - val_loss: 10.0249\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.4614 - val_loss: 10.0246\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.2910 - val_loss: 10.0251\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4237 - val_loss: 10.0248\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6103 - val_loss: 10.0244\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5709 - val_loss: 10.0190\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5879 - val_loss: 10.0188\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.1573 - val_loss: 10.0195\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.9202 - val_loss: 10.0200\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6038 - val_loss: 10.0203\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0082 - val_loss: 10.0196\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4710 - val_loss: 10.0197\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.1456 - val_loss: 10.0219\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.7249 - val_loss: 10.0226\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.7735 - val_loss: 10.0227\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5843 - val_loss: 10.0238\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8310 - val_loss: 10.0258\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.6377 - val_loss: 10.0285\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8644 - val_loss: 10.0293\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.9279 - val_loss: 10.0317\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5789 - val_loss: 10.0342\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.8563 - val_loss: 10.0372\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.8078 - val_loss: 10.0352\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.6227 - val_loss: 10.0346\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.7248 - val_loss: 10.0347\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8311 - val_loss: 10.0343\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2328 - val_loss: 10.0322\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.4226 - val_loss: 10.0333\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5990 - val_loss: 10.0349\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0558 - val_loss: 10.0361\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.3111 - val_loss: 10.0384\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9286 - val_loss: 10.0401\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.7459 - val_loss: 10.0441\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3710 - val_loss: 10.0466\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2147 - val_loss: 10.0478\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.4463 - val_loss: 10.0490\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.6483 - val_loss: 10.0500\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.6959 - val_loss: 10.0516\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1966 - val_loss: 10.0526\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1309 - val_loss: 10.0529\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1640 - val_loss: 10.0546\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 15.8570 - val_loss: 10.0607\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5534 - val_loss: 10.0645\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2799 - val_loss: 10.0660\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9284 - val_loss: 10.0662\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.7907 - val_loss: 10.0676\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1868 - val_loss: 10.0694\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.5681 - val_loss: 10.0709\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.3214 - val_loss: 10.0733\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5204 - val_loss: 10.0762\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2230 - val_loss: 10.0770\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.4368 - val_loss: 10.0777\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.3888 - val_loss: 10.0792\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.6674 - val_loss: 10.0810\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.6226 - val_loss: 10.0812\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.0267 - val_loss: 10.0825\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.6468 - val_loss: 10.0835\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.5621 - val_loss: 10.0843\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5739 - val_loss: 10.0871\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.8310 - val_loss: 10.0878\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.6841 - val_loss: 10.0880\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3818 - val_loss: 10.0875\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3643 - val_loss: 10.0877\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1767 - val_loss: 10.0894\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.0042 - val_loss: 10.0901\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1753 - val_loss: 10.0899\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.7412 - val_loss: 10.0850\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 13.2923 - val_loss: 10.0820\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3251 - val_loss: 10.0803\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.6094 - val_loss: 10.0785\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.1538 - val_loss: 10.0791\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9844 - val_loss: 10.0819\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.4273 - val_loss: 10.0831\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9897 - val_loss: 10.0849\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5414 - val_loss: 10.0873\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8386 - val_loss: 10.0880\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.5478 - val_loss: 10.0883\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.9670 - val_loss: 10.0888\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2926 - val_loss: 10.0889\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.0507 - val_loss: 10.0918\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 14.1608 - val_loss: 10.0942\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4370 - val_loss: 10.0926\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.7141 - val_loss: 10.0980\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.3727 - val_loss: 10.1013\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.0098 - val_loss: 10.1042\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2676 - val_loss: 10.1060\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.9243 - val_loss: 10.1084\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.4438 - val_loss: 10.1115\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.9818 - val_loss: 10.1140\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2334 - val_loss: 10.1146\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2769 - val_loss: 10.1147\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.9045 - val_loss: 10.1134\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.5402 - val_loss: 10.1138\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.9926 - val_loss: 10.1176\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.8700 - val_loss: 10.1214\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.4630 - val_loss: 10.1223\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.4569 - val_loss: 10.1225\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6041 - val_loss: 10.1234\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6181 - val_loss: 10.1250\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2232 - val_loss: 10.1264\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1369 - val_loss: 10.1263\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.9689 - val_loss: 10.1258\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3637 - val_loss: 10.1273\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.6274 - val_loss: 10.1279\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1276 - val_loss: 10.1279\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6912 - val_loss: 10.1283\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.5625 - val_loss: 10.1296\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5001 - val_loss: 10.1295\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1168 - val_loss: 10.1295\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 13.1457 - val_loss: 10.1299\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.1251 - val_loss: 10.1304\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.4774 - val_loss: 10.1302\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.0257 - val_loss: 10.1305\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 13.3179 - val_loss: 10.1303\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 12.9473 - val_loss: 10.1298\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.6562 - val_loss: 10.1300\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.5521 - val_loss: 10.1299\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9097 - val_loss: 10.1302\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.4141 - val_loss: 10.1299\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3013 - val_loss: 10.1297\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.1986 - val_loss: 10.1304\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0200 - val_loss: 10.1308\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2681 - val_loss: 10.1314\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.6513 - val_loss: 10.1322\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1511 - val_loss: 10.1326\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1593 - val_loss: 10.1340\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0232 - val_loss: 10.1338\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.3686 - val_loss: 10.1340\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0757 - val_loss: 10.1339\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.4409 - val_loss: 10.1338\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.6085 - val_loss: 10.1331\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 13.7058 - val_loss: 10.1327\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 12.7560 - val_loss: 10.1326\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.5561 - val_loss: 10.1323\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.4815 - val_loss: 10.1307\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.3165 - val_loss: 10.1308\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 12.8404 - val_loss: 10.1319\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.3694 - val_loss: 10.1328\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.7837 - val_loss: 10.1334\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3141 - val_loss: 10.1336\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2678 - val_loss: 10.1347\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4063 - val_loss: 10.1352\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7182 - val_loss: 10.1356\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 12.8483 - val_loss: 10.1357\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3040 - val_loss: 10.1362\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4316 - val_loss: 10.1381\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2255 - val_loss: 10.1392\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2821 - val_loss: 10.1385\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3495 - val_loss: 10.1385\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.4682 - val_loss: 10.1380\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6184 - val_loss: 10.1386\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.6307 - val_loss: 10.1403\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8785 - val_loss: 10.1404\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5238 - val_loss: 10.1398\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7324 - val_loss: 10.1391\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4769 - val_loss: 10.1392\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3436 - val_loss: 10.1394\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1246 - val_loss: 10.1399\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9198 - val_loss: 10.1401\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2824 - val_loss: 10.1405\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.7434 - val_loss: 10.1414\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0832 - val_loss: 10.1433\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1863 - val_loss: 10.1447\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0541 - val_loss: 10.1456\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.8998 - val_loss: 10.1461\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9118 - val_loss: 10.1462\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.4150 - val_loss: 10.1459\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9345 - val_loss: 10.1460\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9640 - val_loss: 10.1458\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 14.2448 - val_loss: 10.1459\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1452 - val_loss: 10.1456\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8819 - val_loss: 10.1455\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1957 - val_loss: 10.1447\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2071 - val_loss: 10.1446\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7079 - val_loss: 10.1455\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8879 - val_loss: 10.1469\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0956 - val_loss: 10.1461\n",
      "Epoch 412/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1949 - val_loss: 10.1465\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0685 - val_loss: 10.1468\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.4953 - val_loss: 10.1478\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3463 - val_loss: 10.1485\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2017 - val_loss: 10.1498\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0822 - val_loss: 10.1502\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7639 - val_loss: 10.1496\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.5349 - val_loss: 10.1496\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1058 - val_loss: 10.1498\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2893 - val_loss: 10.1502\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9424 - val_loss: 10.1506\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0894 - val_loss: 10.1505\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0623 - val_loss: 10.1514\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0274 - val_loss: 10.1515\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1198 - val_loss: 10.1503\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.9925 - val_loss: 10.1495\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3446 - val_loss: 10.1491\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3867 - val_loss: 10.1496\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.9193 - val_loss: 10.1490\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.6551 - val_loss: 10.1490\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3339 - val_loss: 10.1487\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1186 - val_loss: 10.1479\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8396 - val_loss: 10.1478\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3400 - val_loss: 10.1476\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2529 - val_loss: 10.1484\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2264 - val_loss: 10.1499\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1230 - val_loss: 10.1502\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0997 - val_loss: 10.1502\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1767 - val_loss: 10.1496\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 12.9534 - val_loss: 10.1495\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 13.3692 - val_loss: 10.1497\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0592 - val_loss: 10.1500\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.0776 - val_loss: 10.1497\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.3163 - val_loss: 10.1494\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0876 - val_loss: 10.1478\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3135 - val_loss: 10.1474\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5247 - val_loss: 10.1474\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2788 - val_loss: 10.1468\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2306 - val_loss: 10.1469\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.6674 - val_loss: 10.1465\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9736 - val_loss: 10.1461\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9241 - val_loss: 10.1460\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0998 - val_loss: 10.1458\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1896 - val_loss: 10.1459\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 14.0836 - val_loss: 10.1464\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7296 - val_loss: 10.1469\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1510 - val_loss: 10.1462\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7455 - val_loss: 10.1467\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1073 - val_loss: 10.1473\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8432 - val_loss: 10.1480\n",
      "Epoch 462/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8031 - val_loss: 10.1478\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.5472 - val_loss: 10.1478\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.5473 - val_loss: 10.1475\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.6397 - val_loss: 10.1458\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7257 - val_loss: 10.1458\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3501 - val_loss: 10.1472\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2523 - val_loss: 10.1481\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8693 - val_loss: 10.1476\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0429 - val_loss: 10.1485\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2083 - val_loss: 10.1492\n",
      "Epoch 472/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.4284 - val_loss: 10.1484\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3065 - val_loss: 10.1481\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1081 - val_loss: 10.1497\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2814 - val_loss: 10.1506\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.4213 - val_loss: 10.1514\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7541 - val_loss: 10.1516\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.7592 - val_loss: 10.1525\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.3847 - val_loss: 10.1531\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.5950 - val_loss: 10.1536\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9936 - val_loss: 10.1528\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 12.8193 - val_loss: 10.1514\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7113 - val_loss: 10.1525\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0160 - val_loss: 10.1527\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1326 - val_loss: 10.1542\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1144 - val_loss: 10.1561\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.4865 - val_loss: 10.1561\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 12.8375 - val_loss: 10.1561\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.0301 - val_loss: 10.1567\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.8257 - val_loss: 10.1570\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9383 - val_loss: 10.1573\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0109 - val_loss: 10.1575\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.9663 - val_loss: 10.1576\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0302 - val_loss: 10.1580\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8880 - val_loss: 10.1586\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0189 - val_loss: 10.1577\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.3692 - val_loss: 10.1566\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1243 - val_loss: 10.1568\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8527 - val_loss: 10.1563\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2749 - val_loss: 10.1560\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.5035 - val_loss: 10.1560\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.3637 - val_loss: 10.1555\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7588 - val_loss: 10.1560\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7850 - val_loss: 10.1560\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.6920 - val_loss: 10.1558\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0764 - val_loss: 10.1553\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2585 - val_loss: 10.1549\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7789 - val_loss: 10.1544\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7420 - val_loss: 10.1532\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0454 - val_loss: 10.1531\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9430 - val_loss: 10.1520\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1494 - val_loss: 10.1524\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1303 - val_loss: 10.1524\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7151 - val_loss: 10.1519\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8711 - val_loss: 10.1509\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9521 - val_loss: 10.1501\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9989 - val_loss: 10.1504\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1778 - val_loss: 10.1516\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9278 - val_loss: 10.1515\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.1622 - val_loss: 10.1515\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.6091 - val_loss: 10.1509\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2678 - val_loss: 10.1514\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.2994 - val_loss: 10.1520\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.3616 - val_loss: 10.1517\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8636 - val_loss: 10.1530\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7783 - val_loss: 10.1531\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.7487 - val_loss: 10.1538\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0023 - val_loss: 10.1543\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9315 - val_loss: 10.1549\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.6978 - val_loss: 10.1544\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 13.2496 - val_loss: 10.1537\n",
      "Epoch 532/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.8636 - val_loss: 10.1531\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9527 - val_loss: 10.1543\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1178 - val_loss: 10.1548\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0939 - val_loss: 10.1547\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7766 - val_loss: 10.1549\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.0930 - val_loss: 10.1549\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2186 - val_loss: 10.1554\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8836 - val_loss: 10.1573\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.1469 - val_loss: 10.1589\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.8038 - val_loss: 10.1598\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 11.9335 - val_loss: 10.1620\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.9863 - val_loss: 10.1625\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.6682 - val_loss: 10.1628\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.5895 - val_loss: 10.1641\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8033 - val_loss: 10.1649\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1535 - val_loss: 10.1644\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9113 - val_loss: 10.1657\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0083 - val_loss: 10.1663\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.3596 - val_loss: 10.1666\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8633 - val_loss: 10.1663\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7636 - val_loss: 10.1672\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9335 - val_loss: 10.1681\n",
      "Epoch 554/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7786 - val_loss: 10.1690\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.0625 - val_loss: 10.1697\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2145 - val_loss: 10.1709\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8522 - val_loss: 10.1723\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7642 - val_loss: 10.1719\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.1387 - val_loss: 10.1720\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7894 - val_loss: 10.1711\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.8716 - val_loss: 10.1706\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.3063 - val_loss: 10.1721\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.3488 - val_loss: 10.1739\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9216 - val_loss: 10.1756\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9703 - val_loss: 10.1735\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.6537 - val_loss: 10.1715\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0458 - val_loss: 10.1713\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.1852 - val_loss: 10.1709\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.0383 - val_loss: 10.1727\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.7893 - val_loss: 10.1744\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.3624 - val_loss: 10.1761\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.3224 - val_loss: 10.1767\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.1963 - val_loss: 10.1776\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 12.8626 - val_loss: 10.1778\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 13.1695 - val_loss: 10.1766\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 12.3498 - val_loss: 10.1769\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.9711 - val_loss: 10.1774\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.6635 - val_loss: 10.1776\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 13.0620 - val_loss: 10.1776\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 12.3422 - val_loss: 10.1786\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 13.1767 - val_loss: 10.1805\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.4286 - val_loss: 10.1792\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9859 - val_loss: 10.1750\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7424 - val_loss: 10.1736\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.6570 - val_loss: 10.1736\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.9489 - val_loss: 10.1739\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7463 - val_loss: 10.1731\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.7808 - val_loss: 10.1726\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.6455 - val_loss: 10.1716\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8841 - val_loss: 10.1708\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.8027 - val_loss: 10.1715\n",
      "Epoch 592/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 13.2893 - val_loss: 10.1730\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.6539 - val_loss: 10.1735\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2245 - val_loss: 10.1761\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.9351 - val_loss: 10.1770\n",
      "Epoch 596/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 7.1102"
     ]
    }
   ],
   "source": [
    "best_model=getBestModel(train_data,target_data,\"recommend_stock\")\n",
    "best_model.fit(train_data, target_data, epochs = 1000, validation_data = (test_train_data, test_target_data))\n",
    "best_model.save(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_stock_best_model=getBestModel(only_stock_train_data,only_stock_target_data,\"only_stock_recommend_stock\")\n",
    "only_stock_best_model.fit(only_stock_train_data, only_stock_target_data, epochs = 1000, validation_data = (only_stock_test_train_data, only_stock_test_target_data))\n",
    "only_stock_best_model.save(\"only_stock_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63300351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accOfUpDown(pred,real):\n",
    "    #if same sign, multiply num is positive num\n",
    "    isSame=pred*real\n",
    "    isSame=isSame[isSame>0]\n",
    "    return isSame.shape[0]/pred.shape[0]*100\n",
    "\n",
    "def virtual_invest(pred,real):\n",
    "    money=100\n",
    "    isHave=False\n",
    "    for i in range(0,pred.shape[0]):\n",
    "        \n",
    "        if money<0:\n",
    "            return money\n",
    "        \n",
    "        if pred[i]<0:\n",
    "            if(isHave):\n",
    "                isHave=False\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        elif pred[i]>0:\n",
    "            if(isHave):\n",
    "                money=money+money*real[i]/100\n",
    "            else:\n",
    "                isHave=True\n",
    "                money=money+money*real[i]/100\n",
    "    \n",
    "\n",
    "    return money\n",
    "from matplotlib import pyplot as plt\n",
    "def plotXY(pred,real):\n",
    "    plt.figure(figsize = (20, 10))\n",
    "    plt.plot(pred)\n",
    "    plt.plot(real)\n",
    "    plt.title('Prediction vs Real Stock')\n",
    "    plt.ylabel('ups and downs')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'], loc='best')\n",
    "    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "def printEval(model_name,test_target_data,test_train_data):\n",
    "    saved_model = load_model(model_name, compile = False)\n",
    "    print(saved_model)\n",
    "\n",
    "    predict_test_train_data = saved_model.predict(test_train_data)\n",
    "    predict_test_train_data = predict_test_train_data.flatten()\n",
    "\n",
    "    error_lstm = mean_squared_error(test_target_data, predict_test_train_data)\n",
    "    print(\"MSE Error is\", error_lstm)\n",
    "\n",
    "    upDownAcc=accOfUpDown(predict_test_train_data,test_target_data)\n",
    "    print(upDownAcc,\"%\")\n",
    "\n",
    "    virtual_money=virtual_invest(predict_test_train_data,test_target_data)\n",
    "    print(virtual_money,\"$\")\n",
    "\n",
    "    plotXY(predict_test_train_data,test_target_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x0000017922B30F88>\n",
      "MSE Error is 10.195471621331762\n",
      "57.14285714285714 %\n",
      "183.16666475965263 $\n"
     ]
    }
   ],
   "source": [
    "\n",
    "printEval('best_model.h5',test_target_data, test_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "printEval('only_stock_best_model.h5',only_stock_test_target_data, only_stock_test_train_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9025820792687d1c0da7b36e0715d77bd799d253179141298c77d4d0bf6c43dc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
